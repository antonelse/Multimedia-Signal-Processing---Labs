%% MMSP2 - Lab 1
%  Exercise 1 - Audio signal encoding

clearvars
close all
clc

%% 1) Load file 'pf.wav' and plot it.
%%    Check that its values are included between [-1,+1]
[x,Fs] = audioread('pf.wav');

figure()
plot(x)
ylabel('Amplitude')
xlabel('Samples')

disp(['Fs: ' num2str(Fs)]);
disp(['size(x): ' mat2str(size(x))]);
fprintf('min(x): %.1f\nmax(x): %.1f\n',min(x),max(x));

%% 2) Take the first 60 s of the file and rescale its values
%% between [0 255], rounding to the nearest integer
dur = 60;
x60 = x(1:Fs*dur);
disp(['size(x60): ' mat2str(size(x60))]);

min_in = -1;
max_in = 1;
min_out = 0;
max_out = 255;

x60_255 = round((x60-min_in)*(max_out-min_out)/(max_in-min_in)+min_out);
%%Given the signal ùê±, you can scale the values of ùê± to fall on the interval [ùëé,ùëè] with
%%x_scaled = (x-min(x))*(b-a)/(max(x)-min(x)) + a;


fprintf('min(x60_255): %.1f\nmax(x60_255): %.1f\n',min(x60_255),max(x60_255));

%% 3) Convert each value into its binary representation over 8 bit
%%    hint: use the function de2bi() or dec2bin() - '0' if you don't have the Communications System toolbox
x60_255_bi = de2bi(x60_255);
%la trasf del sample 8bit √® implicito poich√® ho imposto la sua dinamica a 256(2^8)
%x60_255_bi = dec2bin(x60_255) - '0';

disp(['size(x60_255_bi): ' mat2str(size(x60_255_bi))]);

%% 4) Display the entropy of the binary source that has generated the signal
%%    hint: use the function hist() to compute the probability distribution
alphabet_bi = 0:1;
d_bi = hist(x60_255_bi(:),alphabet_bi);
disp(['size(d_bi): ' mat2str(size(d_bi))]);

p_bi = d_bi/sum(d_bi);
%probabilit√† del segnale binario: valore/somma di tutti

figure()
bar(alphabet_bi, p_bi);
xlabel('Symbol');
ylabel('Probability');
grid('on');

H_bi = -sum(p_bi .* log2(p_bi));

fprintf('entropy: %.3f bit/symbol\n',H_bi);

%% 5) Consider the signal as generated by a finite source with alphabet [0 255].
%%    Plot the normalized histogram and compute the entropy.
%%    hint: avoid evaluating log2(0) when computing the entropy! (√® infinito)
%%stessa cosa di prima
alphabet_255 = 0:255;
d_255 = hist(x60_255(:),alphabet_255);
disp(['size(d_255): ' mat2str(size(d_255))]);

p_255 = d_255/sum(d_255);

figure()
bar(alphabet_255, p_255);
xlabel('Symbol');
ylabel('Probability');
grid('on');

H_255 = -sum(p_255(d_255>0) .* log2(p_255(d_255>0)));
%p_255 o d_255 > 0 √® la stessa cosa dato che se d √® 0 anche p lo √®
%imposto il >0 perch√® voglio evitare la computazione di log(0) !!

fprintf('entropy: %.3f bit/symbol <= 8 bit/symbol\n',H_255);
% %.3f significa max 3 float dopo la virgola

%% 6) Consider the signal as generated by a source with memory=1.
%%    Compute the conditional entropy

j = x60_255(1:end-1);
k = x60_255(2:end);
 %sono 2 variabili che si overlappano sempre tranne in testa e in coda
 %serve per definire la sorgente con memoria 1

% Compute joint probability

d_joint = hist3([j, k],{alphabet_255,alphabet_255});
%hist3 Three-dimensional histogram of bivariate data.
disp(['size(d_joint): ' mat2str(size(d_joint))]);

p_joint = d_joint/sum(d_joint(:));
%Nb: √® una matrice

figure();
imagesc(db(p_joint)), axis xy;
%ho rappresentato la prob congiunta tra i sample di k e di j. La retta mi indica
%che i sample vicini (nota come √® definita k e j, c'√® solo un sample di differenza) sono molto correlati
%infatti retta a 45 gradi
%imagesc Display image with scaled colors
title('Joint PMF');
ylabel('j symbols');
xlabel('k symbols');

% compute joint entropy
H_joint = -sum(sum(p_joint(d_joint>0) .* log2(p_joint(d_joint>0))));
fprintf('joint entropy: %.3f bit/2 symbols <= 16 bit/2 symbols\n',H_joint);

% compute conditional entropy (K|J) using chain rule(quindi mi manca solo H_j come dato)
d_j = hist(j(:),alphabet_255);
p_j = d_j/sum(d_j);
H_j = -sum(p_j(d_j>0) .* log2(p_j(d_j>0)));
%QUESTE TRE RIGHE SONO MOLTO RICORRENTI !!!

H_cond_cr = H_joint - H_j;
fprintf('cond entropy by chain rule: %.3f bit/symbol\n',H_cond_cr);

% compute conditional entropy by definition
p_j_ext = repmat(p_j,256,1);
%B = repmat(A,n,m) ripete la matrice A per n righe (una sotto l'altra, dall'alto in basso) e m colonne (da sx a dx)
%ATTENZIONE, p_j era un vettore, e facendo questo creo una big matrice che ha 256 righe (copie di p_j una sotto l'altra)
%e una sola ripetizione in colonna, quindi 256 colonne (aka lunghezza di p_j, presa solo una volta)
non_zero_mask = p_joint > 0;
%solita condizione per evitare log(0) ma in questo caso la chiamo maschera dato che i valori sono gli elementi di una matrice
%p_j_ext mi √® servito per fare l'operazione di divisione elementwise (./) qui sotto, in accordo con la definizione di entropia condiz
%dovendo fare il log(p_joint/p_j) e p_joint √® una matrice, ho esteso a matrice la p_j (p_j_ext) cos√¨ da poter fare operazioni element wise ./,
%altrimenti avrei dovuto fare un ciclo for per dividere una matrice con un vettore

H_cond_def = -sum(sum(p_joint(non_zero_mask) .* log2(p_joint(non_zero_mask)./p_j_ext(non_zero_mask))));
fprintf('cond entropy by definition: %.3f bit/symbol\n',H_cond_def);
